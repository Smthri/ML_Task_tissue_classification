{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"problem_1_starter.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"atqZGIIyNSBb"},"source":["#**Практическое задание №1**"]},{"cell_type":"markdown","metadata":{"id":"ga5g3lUhNNBy"},"source":["Установка необходимых пакетов:"]},{"cell_type":"code","metadata":{"id":"TGBk36LpukIu","executionInfo":{"status":"ok","timestamp":1612632716924,"user_tz":-180,"elapsed":5640,"user":{"displayName":"Live It","photoUrl":"","userId":"01143106410518725341"}}},"source":["!pip install -q libtiff\n","!pip install -q tqdm"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2vQDLyHEO1Ux"},"source":["Монтирование Вашего Google Drive к текущему окружению:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5G5KkA1Nu5M9","executionInfo":{"status":"ok","timestamp":1612632717925,"user_tz":-180,"elapsed":6624,"user":{"displayName":"Live It","photoUrl":"","userId":"01143106410518725341"}},"outputId":"371984fe-c23a-4557-8294-295710c32312"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"E6-mtI6W1y1b"},"source":["В переменную PROJECT_DIR необходимо прописать путь к директории на Google Drive, в которую Вы загрузили zip архивы с предоставленными наборами данных."]},{"cell_type":"code","metadata":{"id":"IdvM-BUTvfSV","executionInfo":{"status":"ok","timestamp":1612632717926,"user_tz":-180,"elapsed":6615,"user":{"displayName":"Live It","photoUrl":"","userId":"01143106410518725341"}}},"source":["# todo\n","PROJECT_DIR = 'Хвостиков_задание/'"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Num5lHV6912"},"source":["Константы, которые пригодятся в коде далее:"]},{"cell_type":"code","metadata":{"id":"ab2yCwDm7Fqb","executionInfo":{"status":"ok","timestamp":1612632717927,"user_tz":-180,"elapsed":6609,"user":{"displayName":"Live It","photoUrl":"","userId":"01143106410518725341"}}},"source":["EVALUATE_ONLY = False\r\n","TEST_ON_LARGE_DATASET = False\r\n","TISSUE_CLASSES = ('ADI', 'BACK', 'DEB', 'LYM', 'MUC', 'MUS', 'NORM', 'STR', 'TUM')"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fgY-ux5qOI0k"},"source":["Импорт необходимых зависимостей:"]},{"cell_type":"code","metadata":{"id":"kLHQhqiSIyvK","executionInfo":{"status":"ok","timestamp":1612632718870,"user_tz":-180,"elapsed":7544,"user":{"displayName":"Live It","photoUrl":"","userId":"01143106410518725341"}}},"source":["from pathlib import Path\n","from libtiff import TIFF\n","import numpy as np\n","from typing import List\n","from tqdm.notebook import tqdm\n","from time import sleep\n","from PIL import Image\n","import IPython.display\n","from sklearn.metrics import balanced_accuracy_score"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zKLI3lUyMYO9"},"source":["---\n","### Класс Dataset\n","\n","Предназначен для работы с наборами данных, хранящихся на Google Drive, обеспечивает чтение изображений и соответствующих меток, а также формирование пакетов (батчей)."]},{"cell_type":"code","metadata":{"id":"8N169efsw1ej","executionInfo":{"status":"ok","timestamp":1612632718874,"user_tz":-180,"elapsed":7538,"user":{"displayName":"Live It","photoUrl":"","userId":"01143106410518725341"}}},"source":["class Dataset:\r\n","\r\n","    def __init__(self, name, gdrive_dir):\r\n","        self.name = name\r\n","        self.is_loaded = False\r\n","        p = Path(\"/content/drive/MyDrive/\" + gdrive_dir + name + '.npz') \r\n","        if p.exists():\r\n","            print(f'Loading dataset {self.name} from npz.')\r\n","            np_obj = np.load(str(p))\r\n","            self.images = np_obj['data']\r\n","            self.labels = np_obj['labels']\r\n","            self.n_files = self.images.shape[0]\r\n","            self.is_loaded = True\r\n","            print(f'Done. Dataset {name} consists of {self.n_files} images.')\r\n","        else:\r\n","          print('WARNING: path doesn\\'t exist')\r\n","\r\n","    def image(self, i):\r\n","        # read i-th image in dataset and return it as numpy array\r\n","        if self.is_loaded:\r\n","            return self.images[i, :, :, :]\r\n","\r\n","    def images_seq(self, start=0, n=None):\r\n","        # sequential access to images inside dataset (is needed for testing)\r\n","        for i in range(start, start + self.n_files if not n else start + n):\r\n","            yield self.image(i)\r\n","\r\n","    def random_image_with_label(self):\r\n","        # get random image with label from dataset\r\n","        i = np.random.randint(self.n_files)\r\n","        return self.image(i), self.labels[i]\r\n","  \r\n","    def random_batch_with_labels(self, n):\r\n","        # create random batch of images with labels (is needed for training)\r\n","        indices = np.random.choice(self.n_files, n)\r\n","        imgs = []\r\n","        for i in indices:\r\n","            img = self.image(i)\r\n","            imgs.append(self.image(i))\r\n","        logits = np.array([self.labels[i] for i in indices])\r\n","        return np.stack(imgs), logits\r\n","\r\n","    def image_with_label(self, i: int):\r\n","        # return i-th image with label from dataset\r\n","        return self.image(i), self.labels[i]"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M-LvGqeHYgus"},"source":["### Пример использвания класса Dataset\n","Загрузим обучающий набор данных, получим произвольное изображение с меткой. После чего визуализируем изображение, выведем метку. В будущем, этот кусок кода можно закомментировать или убрать."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"HhObWEjGJ1um","executionInfo":{"status":"ok","timestamp":1612632718875,"user_tz":-180,"elapsed":7524,"user":{"displayName":"Live It","photoUrl":"","userId":"01143106410518725341"}},"outputId":"339f2eac-58ab-4b6b-e033-3c944f1339f9"},"source":["'''d_train_tiny = Dataset('train_tiny', PROJECT_DIR)\n","\n","img, lbl = d_train_tiny.random_image_with_label()\n","print()\n","print(f'Got numpy array of shape {img.shape}, and label with code {lbl}.')\n","print(f'Label code corresponds to {TISSUE_CLASSES[lbl]} class.')\n","\n","pil_img = Image.fromarray(img)\n","IPython.display.display(pil_img)'''"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"d_train_tiny = Dataset('train_tiny', PROJECT_DIR)\\n\\nimg, lbl = d_train_tiny.random_image_with_label()\\nprint()\\nprint(f'Got numpy array of shape {img.shape}, and label with code {lbl}.')\\nprint(f'Label code corresponds to {TISSUE_CLASSES[lbl]} class.')\\n\\npil_img = Image.fromarray(img)\\nIPython.display.display(pil_img)\""]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"qaBXXCWeVLYb"},"source":["---\n","### Класс Metrics\n","\n","Реализует метрики точности, используемые для оценивания модели:\n","1. точность,\n","2. сбалансированную точность."]},{"cell_type":"code","metadata":{"id":"5unQ7azTinCZ","executionInfo":{"status":"ok","timestamp":1612632718876,"user_tz":-180,"elapsed":7512,"user":{"displayName":"Live It","photoUrl":"","userId":"01143106410518725341"}}},"source":["class Metrics:\n","\n","    @staticmethod\n","    def accuracy(gt: List[int], pred: List[int]):\n","        assert len(gt) == len(pred), 'gt and prediction should be of equal length'\n","        print([f'{i[0]} : {i[1]}' for i in list(zip(gt, pred))[:5]])\n","        return sum(int(i[0] == i[1]) for i in zip(gt, pred)) / len(gt)\n","\n","    @staticmethod\n","    def accuracy_balanced(gt: List[int], pred: List[int]):\n","        return balanced_accuracy_score(gt, pred)\n","\n","    @staticmethod\n","    def print_all(gt: List[int], pred: List[int], info: str):\n","        print(f'metrics for {info}:')\n","        print('\\t accuracy {:.4f}:'.format(Metrics.accuracy(gt, pred)))\n","        print('\\t balanced accuracy {:.4f}:'.format(Metrics.accuracy_balanced(gt, pred)))"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N1AHzTJVGU5k"},"source":["---\n","### Класс Model\n","\n","Класс, хранящий в себе всю информацию о модели.\n","\n","Вам необходимо реализовать методы save, load для сохранения и заргрузки модели. Особенно актуально это будет во время тестирования на дополнительных наборах данных.\n","\n","> *Пожалуйста, убедитесь, что сохранение и загрузка модели работает корректно. Для этого обучите модель, протестируйте, сохраните ее в файл, перезапустите среду выполнения, загрузите обученную модель из файла, вновь протестируйте ее на тестовой выборке и убедитесь в том, что получаемые метрики совпадают с полученными для тестовой выбрки ранее.*\n","\n","\n","Также, Вы можете реализовать дополнительные функции, такие как:\n","1. валидацию модели на части обучающей выборки;\n","2. использование кроссвалидации;\n","3. автоматическое сохранение модели при обучении;\n","4. загрузку модели с какой-то конкретной итерации обучения (если используется итеративное обучение);\n","5. вывод различных показателей в процессе обучения (например, значение функции потерь на каждой эпохе);\n","6. построение графиков, визуализирующих процесс обучения (например, график зависимости функции потерь от номера эпохи обучения);\n","7. автоматическое тестирование на тестовом наборе/наборах данных после каждой эпохи обучения (при использовании итеративного обучения);\n","8. автоматический выбор гиперпараметров модели во время обучения;\n","9. сохранение и визуализацию результатов тестирования;\n","10. Использование аугментации и других способов синтетического расширения набора данных (дополнительным плюсом будет обоснование необходимости и обоснование выбора конкретных типов аугментации)\n","11. и т.д.\n","\n","Полный список опций и дополнений приведен в презентации с описанием задания.\n","\n","При реализации дополнительных функций допускается добавление параметров в существующие методы и добавление новых методов в класс модели."]},{"cell_type":"code","metadata":{"id":"0pkMiB6mJ7JQ","executionInfo":{"status":"ok","timestamp":1612632725232,"user_tz":-180,"elapsed":13859,"user":{"displayName":"Live It","photoUrl":"","userId":"01143106410518725341"}}},"source":["import tensorflow as tf\n","from tensorflow.keras.regularizers import l2\n","import cv2\n","import os\n","from random import random, randint\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import SGDClassifier\n","\n","class Model:\n","\n","    def __init__(self):\n","        # Using architecture from https://openaccess.thecvf.com/content_cvpr_2016/papers/Hou_Patch-Based_Convolutional_Neural_CVPR_2016_paper.pdf\n","        \n","        self.input_shape=(512, 512, 3)\n","        self.model = tf.keras.Sequential([\n","          tf.keras.layers.Conv2D(filters=16, kernel_size=3, input_shape=self.input_shape),\n","          tf.keras.layers.ReLU(),\n","          tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=3),\n","\n","          tf.keras.layers.Conv2D(filters=32, kernel_size=3),\n","          tf.keras.layers.ReLU(),\n","          tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2),\n","          tf.keras.layers.Conv2D(filters=64, kernel_size=3),\n","          tf.keras.layers.ReLU(),\n","          tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2),\n","\n","          tf.keras.layers.Conv2D(filters=64, kernel_size=3),\n","          tf.keras.layers.ReLU(),\n","          tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=3),\n","          tf.keras.layers.Conv2D(filters=32, kernel_size=3),\n","          tf.keras.layers.ReLU(),\n","          tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=3),\n","\n","          tf.keras.layers.Flatten(),\n","          tf.keras.layers.Dense(units=256, activation='relu'),\n","          tf.keras.layers.Dense(units=128, activation='relu'),\n","          tf.keras.layers.Dense(units=9, activation='softmax')\n","        ])\n","        '''self.model = tf.keras.Sequential([\n","          tf.keras.layers.Conv2D(filters=80, kernel_size=5, strides=2, input_shape=self.input_shape, kernel_regularizer=l2(.0005)),\n","          tf.keras.layers.BatchNormalization(),\n","          tf.keras.layers.ReLU(),\n","          tf.keras.layers.MaxPool2D(pool_size=(6, 6), strides=2),\n","\n","          tf.keras.layers.Conv2D(filters=120, kernel_size=5, kernel_regularizer=l2(.0005)),\n","          tf.keras.layers.BatchNormalization(),\n","          tf.keras.layers.ReLU(),\n","          tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n","\n","          tf.keras.layers.Conv2D(filters=160, kernel_size=3, kernel_regularizer=l2(.0005)),\n","          tf.keras.layers.ReLU(),\n","\n","          tf.keras.layers.Conv2D(filters=200, kernel_size=3, kernel_regularizer=l2(.0005)),\n","          tf.keras.layers.ReLU(),\n","          tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n","\n","          tf.keras.layers.Flatten(),\n","\n","          tf.keras.layers.Dense(units=256, activation='relu'),\n","          tf.keras.layers.Dropout(rate=0.5),\n","\n","          tf.keras.layers.Dense(units=256, activation='relu'),\n","          tf.keras.layers.Dropout(rate=0.5),\n","\n","          tf.keras.layers.Dense(units=9, activation='softmax')\n","        ])'''\n","        #self.model = tf.keras.applications.InceptionV3(classes=9, weights=None)\n","\n","        self.save_path='drive/MyDrive/' + PROJECT_DIR + '/best.h5'\n","        self.checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","            filepath=self.save_path,\n","            monitor='val_accuracy',\n","            save_best_only=True,\n","            verbose=True\n","        )\n","\n","        self.model.compile(optimizer='adam',\n","                           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","                           metrics=['accuracy'])\n","        \n","        self.clf = SGDClassifier()\n","        #self.clf = make_pipeline(StandardScaler(), SGDClassifier())\n","\n","    def summary(self):\n","      print(self.model.summary())\n","\n","    def save(self, name: str):\n","        # save model to PROJECT_DIR folder on gdrive with name 'name'\n","        # todo\n","        pass\n","\n","    @staticmethod\n","    def load(name: str):\n","        # load model with name 'name' from PROJECT_DIR folder on gdrive\n","        # todo\n","        model = Model()\n","        if os.path.exists('drive/MyDrive/' + PROJECT_DIR + f'/{name}.h5'):\n","          model.model = tf.keras.models.load_model('drive/MyDrive/' + PROJECT_DIR + f'/{name}.h5')\n","        return model\n","\n","    def train(self, dataset: Dataset):\n","        # you can add some plots for better visualization,\n","        # you can add model autosaving during training,\n","        # etc.\n","        print(f'training started')\n","        \n","        for i in range(70):\n","\n","          n = 500\n","          x_train, y_train = dataset.random_batch_with_labels(n)\n","          x_train = self.preproc_batch(x_train)\n","\n","          x_val, y_val = dataset.random_batch_with_labels(150)\n","          x_val = self.preproc_batch(x_val, 0)\n","\n","          print(x_train.shape, y_train.shape)\n","          self.model.fit(x_train, y_train, epochs=30, callbacks=[self.checkpoint_callback], validation_data=(x_val, y_val))\n","\n","        print(f'training done')\n","        pass\n","\n","    def train_clf(self, dataset: Dataset):\n","      print('training_clf started')\n","\n","      intermediate_layer = 'dropout_1'\n","      self.feature_extractor = tf.keras.models.Model(inputs=self.model.input, outputs=self.model.get_layer(intermediate_layer).output)\n","\n","      for i in tqdm(range(70)):\n","\n","        n = 250\n","        x_train, y_train = dataset.random_batch_with_labels(n)\n","        x_train = self.preproc_batch(x_train)\n","        print(x_train.shape)\n","        x_train = self.feature_extractor.predict(x_train)\n","\n","        print(x_train.shape)\n","\n","        self.clf.partial_fit(x_train, y_train, classes=np.arange(9))\n","\n","      print('training_clf done')\n","\n","    def preproc_batch(self, batch, thr=.5):\n","      imgs = []\n","      for image in batch:\n","        if random() < thr:\n","          border = randint(10, 40)\n","          image = image[border:-border, border:-border]\n","        img = cv2.resize(image/255., self.input_shape[:2], interpolation=cv2.INTER_CUBIC)\n","        if random() < thr:\n","          img = img[:, ::-1]\n","        if random() < thr:\n","          img = img[::-1]\n","        imgs.append(img)\n","\n","      return np.stack(imgs)\n","\n","    def test_on_dataset(self, dataset: Dataset, limit=None, use_clf=False):\n","        # you can upgrade this code if you want to speed up testing using batches\n","        predictions = []\n","        n = dataset.n_files if not limit else int(dataset.n_files * limit)\n","        batch_size = 100\n","        current = 0\n","        for current in tqdm(range(0, n, min(batch_size, n - current))):\n","          for img in self.preproc_batch(dataset.images_seq(current, batch_size)):\n","            predictions.append(self.test_on_image(img[np.newaxis, :], use_clf))\n","        return predictions\n","\n","    def test_on_image(self, img: np.ndarray, use_clf=False):\n","      if use_clf:\n","        prediction = self.clf.predict(self.feature_extractor.predict(img))\n","        return prediction\n","      else:\n","        prediction = self.model.predict(img)\n","        return np.argmax(prediction)\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZMpTB6lMr00A"},"source":["---\r\n","### Классификация изображений\r\n","\r\n","Используя введенные выше классы можем перейти уже непосредственно к обучению модели классификации изображений. Пример общего пайплайна решения задачи приведен ниже. Вы можете его расширять и улучшать. В данном примере используются наборы данных 'train_small' и 'test_small'."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5cTOuZD01Up6","executionInfo":{"status":"ok","timestamp":1612632797705,"user_tz":-180,"elapsed":86321,"user":{"displayName":"Live It","photoUrl":"","userId":"01143106410518725341"}},"outputId":"e0d6d981-a020-41ad-f1c9-6b894e82dafa"},"source":["d_train = Dataset('train', PROJECT_DIR)\r\n","d_test = Dataset('test', PROJECT_DIR)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Loading dataset train from npz.\n","Done. Dataset train consists of 18000 images.\n","Loading dataset test from npz.\n","Done. Dataset test consists of 4500 images.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wBi0XpXg8_wq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3c27f7a0-a2d2-49a7-c431-79c26db066ed"},"source":["model = Model.load('best')\n","model.summary()\n","if not EVALUATE_ONLY:\n","  #model.train_clf(d_train)\n","  model.train(d_train)\n","\n","  model.load('best')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 510, 510, 16)      448       \n","_________________________________________________________________\n","re_lu (ReLU)                 (None, 510, 510, 16)      0         \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 170, 170, 16)      0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 168, 168, 32)      4640      \n","_________________________________________________________________\n","re_lu_1 (ReLU)               (None, 168, 168, 32)      0         \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 84, 84, 32)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 82, 82, 64)        18496     \n","_________________________________________________________________\n","re_lu_2 (ReLU)               (None, 82, 82, 64)        0         \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 41, 41, 64)        0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 39, 39, 64)        36928     \n","_________________________________________________________________\n","re_lu_3 (ReLU)               (None, 39, 39, 64)        0         \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 13, 13, 64)        0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 11, 11, 32)        18464     \n","_________________________________________________________________\n","re_lu_4 (ReLU)               (None, 11, 11, 32)        0         \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 3, 3, 32)          0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 288)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 256)               73984     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 128)               32896     \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 9)                 1161      \n","=================================================================\n","Total params: 187,017\n","Trainable params: 187,017\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","training started\n","(500, 512, 512, 3) (500,)\n","Epoch 1/30\n","16/16 [==============================] - ETA: 0s - loss: 2.2037 - accuracy: 0.0962"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CcM2EiRMVP93"},"source":["Пример тестирования модели на части набора данных:"]},{"cell_type":"code","metadata":{"id":"I0AqmeLEKqrs"},"source":["# evaluating model on x% of test dataset\r\n","limit=1.\r\n","pred_1 = model.test_on_dataset(d_test, limit=limit)\r\n","Metrics.print_all(d_test.labels[:len(pred_1)], pred_1, f'{limit*100}% of test')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mSwvHVVzVWZ5"},"source":["Пример тестирования модели на полном наборе данных:"]},{"cell_type":"code","metadata":{"id":"mjI_sbMi3TMY"},"source":["# evaluating model on full test dataset (may take time)\r\n","if TEST_ON_LARGE_DATASET:\r\n","    pred_2 = model.test_on_dataset(d_test)\r\n","    Metrics.print_all(d_test.labels, pred_2, 'test')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kvyEHdxEB18o"},"source":["Результат работы пайплайна обучения и тестирования выше тоже будет оцениваться. Поэтому не забудьте присылать на проверку ноутбук с выполнеными ячейками кода с демонстрациями метрик обучения, графиками и т.п. В этом пайплайне Вам необходимо продемонстрировать работу всех реализованных дополнений, улучшений и т.п.\r\n","\r\n","<font color=\"red\">\r\n","Настоятельно рекомендуется после получения пайплайна с полными результатами обучения экспортировать ноутбук в pdf (файл -> печать) и прислать этот pdf вместе с самим ноутбуком.\r\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"RzSKAvVI6uCW"},"source":["### Тестирование модели на других наборах данных\r\n","\r\n","Ваша модель должна поддерживать тестирование на других наборах данных. Для удобства, Вам предоставляется набор данных test_tiny, который представляет собой малую часть (2% изображений) набора test. Ниже приведен фрагмент кода, который будет осуществлять тестирование для оценивания Вашей модели на дополнительных тестовых наборах данных.\r\n","\r\n","<font color=\"red\">\r\n","Прежде чем отсылать задание на проверку, убедитесь в работоспособности фрагмента кода ниже.\r\n","</font>"]},{"cell_type":"code","metadata":{"id":"sdY3uTt87tqv"},"source":["final_model = Model()\r\n","final_model.load('best')\r\n","d_test_tiny = Dataset('test_tiny', PROJECT_DIR)\r\n","pred = model.test_on_dataset(d_test_tiny)\r\n","Metrics.print_all(d_test_tiny.labels, pred, 'test-tiny')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lPvyj4gscU10"},"source":["Отмонтировать Google Drive."]},{"cell_type":"code","metadata":{"id":"NfX35zNSvFWn"},"source":["#drive.flush_and_unmount()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RMyDxCDCspcI"},"source":["---\r\n","# Дополнительные \"полезности\"\r\n","\r\n","Ниже приведены примеры использования различных функций и библиотек, которые могут быть полезны при выполнении данного практического задания."]},{"cell_type":"markdown","metadata":{"id":"VvLwSttCs1rB"},"source":["### Измерение времени работы кода\r\n","\r\n","Измерять время работы какой-либо функции можно легко и непринужденно при помощи функции timeit из соответствующего модуля:"]},{"cell_type":"code","metadata":{"id":"-HnLVhwE9C9S"},"source":["'''import timeit\n","\n","def factorial(n):\n","    res = 1\n","    for i in range(1, n + 1):\n","        res *= i\n","    return res\n","\n","\n","def f():\n","    return factorial(n=1000)\n","\n","n_runs = 128\n","print(f'Function f is caluclated {n_runs} times in {timeit.timeit(f, number=n_runs)}s.')'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fibGVEdguOOi"},"source":["### Scikit-learn\r\n","\r\n","Для использования \"классических\" алгоритмов машинного обучения рекомендуется использовать библиотеку scikit-learn (https://scikit-learn.org/stable/). Пример классификации изображений цифр из набора данных MNIST при помощи классификатора SVM:"]},{"cell_type":"code","metadata":{"id":"vXHnBzEfunAO"},"source":["'''# Standard scientific Python imports\r\n","import matplotlib.pyplot as plt\r\n","\r\n","# Import datasets, classifiers and performance metrics\r\n","from sklearn import datasets, svm, metrics\r\n","from sklearn.model_selection import train_test_split\r\n","\r\n","# The digits dataset\r\n","digits = datasets.load_digits()\r\n","\r\n","# The data that we are interested in is made of 8x8 images of digits, let's\r\n","# have a look at the first 4 images, stored in the `images` attribute of the\r\n","# dataset.  If we were working from image files, we could load them using\r\n","# matplotlib.pyplot.imread.  Note that each image must have the same size. For these\r\n","# images, we know which digit they represent: it is given in the 'target' of\r\n","# the dataset.\r\n","_, axes = plt.subplots(2, 4)\r\n","images_and_labels = list(zip(digits.images, digits.target))\r\n","for ax, (image, label) in zip(axes[0, :], images_and_labels[:4]):\r\n","    ax.set_axis_off()\r\n","    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\r\n","    ax.set_title('Training: %i' % label)\r\n","\r\n","# To apply a classifier on this data, we need to flatten the image, to\r\n","# turn the data in a (samples, feature) matrix:\r\n","n_samples = len(digits.images)\r\n","data = digits.images.reshape((n_samples, -1))\r\n","\r\n","# Create a classifier: a support vector classifier\r\n","classifier = svm.SVC(gamma=0.001)\r\n","\r\n","# Split data into train and test subsets\r\n","X_train, X_test, y_train, y_test = train_test_split(\r\n","    data, digits.target, test_size=0.5, shuffle=False)\r\n","\r\n","# We learn the digits on the first half of the digits\r\n","classifier.fit(X_train, y_train)\r\n","\r\n","# Now predict the value of the digit on the second half:\r\n","predicted = classifier.predict(X_test)\r\n","\r\n","images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\r\n","for ax, (image, prediction) in zip(axes[1, :], images_and_predictions[:4]):\r\n","    ax.set_axis_off()\r\n","    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\r\n","    ax.set_title('Prediction: %i' % prediction)\r\n","\r\n","print(\"Classification report for classifier %s:\\n%s\\n\"\r\n","      % (classifier, metrics.classification_report(y_test, predicted)))\r\n","disp = metrics.plot_confusion_matrix(classifier, X_test, y_test)\r\n","disp.figure_.suptitle(\"Confusion Matrix\")\r\n","print(\"Confusion matrix:\\n%s\" % disp.confusion_matrix)\r\n","\r\n","plt.show()'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uu3Dny5zxcVy"},"source":["### Scikit-image\r\n","\r\n","Реализовывать различные операции для работы с изображениями можно как самостоятельно, работая с массивами numpy, так и используя специализированные библиотеки, например, scikit-image (https://scikit-image.org/). Ниже приведен пример использования Canny edge detector."]},{"cell_type":"code","metadata":{"id":"5TZvy_d7xc0B"},"source":["'''import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","from scipy import ndimage as ndi\r\n","\r\n","from skimage import feature\r\n","\r\n","\r\n","# Generate noisy image of a square\r\n","im = np.zeros((128, 128))\r\n","im[32:-32, 32:-32] = 1\r\n","\r\n","im = ndi.rotate(im, 15, mode='constant')\r\n","im = ndi.gaussian_filter(im, 4)\r\n","im += 0.2 * np.random.random(im.shape)\r\n","\r\n","# Compute the Canny filter for two values of sigma\r\n","edges1 = feature.canny(im)\r\n","edges2 = feature.canny(im, sigma=3)\r\n","\r\n","# display results\r\n","fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(8, 3),\r\n","                                    sharex=True, sharey=True)\r\n","\r\n","ax1.imshow(im, cmap=plt.cm.gray)\r\n","ax1.axis('off')\r\n","ax1.set_title('noisy image', fontsize=20)\r\n","\r\n","ax2.imshow(edges1, cmap=plt.cm.gray)\r\n","ax2.axis('off')\r\n","ax2.set_title(r'Canny filter, $\\sigma=1$', fontsize=20)\r\n","\r\n","ax3.imshow(edges2, cmap=plt.cm.gray)\r\n","ax3.axis('off')\r\n","ax3.set_title(r'Canny filter, $\\sigma=3$', fontsize=20)\r\n","\r\n","fig.tight_layout()\r\n","\r\n","plt.show()'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hiEWhGUQRGoH"},"source":["### Tensorflow 2\r\n","\r\n","Для создания и обучения нейросетевых моделей можно использовать фреймворк глубокого обучения Tensorflow 2. Ниже приведен пример простейшей нейроной сети, использующейся для классификации изображений из набора данных MNIST."]},{"cell_type":"code","metadata":{"id":"kDwLG7A1ReNy"},"source":["'''# Install TensorFlow\r\n","\r\n","import tensorflow as tf\r\n","\r\n","mnist = tf.keras.datasets.mnist\r\n","\r\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n","x_train, x_test = x_train / 255.0, x_test / 255.0\r\n","\r\n","model = tf.keras.models.Sequential([\r\n","  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n","  tf.keras.layers.Dense(128, activation='relu'),\r\n","  tf.keras.layers.Dropout(0.2),\r\n","  tf.keras.layers.Dense(10, activation='softmax')\r\n","])\r\n","\r\n","model.compile(optimizer='adam',\r\n","              loss='sparse_categorical_crossentropy',\r\n","              metrics=['accuracy'])\r\n","\r\n","model.fit(x_train, y_train, epochs=5)\r\n","\r\n","model.evaluate(x_test,  y_test, verbose=2)'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IbvktmLwRu8g"},"source":["<font color=\"red\">\r\n","Для эффективной работы с моделями глубокого обучения убедитесь в том, что в текущей среде Google Colab используется аппаратный ускоритель GPU или TPU. Для смены среды выберите \"среда выполнения\" -> \"сменить среду выполнения\".\r\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"nJVNOOU9Sjyf"},"source":["Большое количество туториалов и примеров с кодом на Tensorflow 2 можно найти на официальном сайте https://www.tensorflow.org/tutorials?hl=ru. "]},{"cell_type":"markdown","metadata":{"id":"GVPs3pYpS0U1"},"source":["Также, Вам может понадобиться написать собственный генератор данных для Tensorflow 2. Скорее всего он будет достаточно простым, и его легко можно будет реализовать, используя официальную документацию TensorFlow 2. Но, на всякий случай (если не удлось сразу разобраться или хочется вникнуть в тему более глубоко), можете посмотреть следующий отличный туториал: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly."]},{"cell_type":"markdown","metadata":{"id":"lwI-T0IXyN84"},"source":["### Numba\r\n","\r\n","В некоторых ситуациях, при ручных реализациях графовых алгоритмов, выполнение многократных вложенных циклов for в python можно существенно ускорить, используя JIT-компилятор Numba (https://numba.pydata.org/).\r\n","Примеры использования Numba в Google Colab можно найти тут:\r\n","1. https://colab.research.google.com/github/cbernet/maldives/blob/master/numba/numba_cuda.ipynb\r\n","2. https://colab.research.google.com/github/evaneschneider/parallel-programming/blob/master/COMPASS_gpu_intro.ipynb \r\n","\r\n","> Пожалуйста, если Вы решили использовать Numba для решения этого практического задания, еще раз подумайте, нужно ли это Вам, и есть ли возможность реализовать требуемую функциональность иным способом. Используйте Numba только при реальной необходимости.\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"BxAJ00A76LcF"},"source":["### Работа с zip архивами в Google Drive\r\n","\r\n","Запаковка и распаковка zip архивов может пригодиться при сохранении и загрузки Вашей модели. Ниже приведен фрагмент кода, иллюстрирующий помещение нескольких файлов в zip архив с последующим чтением файлов из него. Все действия с директориями, файлами и архивами должны осущетвляться с примонтированным Google Drive.\r\n"]},{"cell_type":"markdown","metadata":{"id":"ZJiKndOpPu_e"},"source":["Создадим 2 изображения, поместим их в директорию tmp внутри PROJECT_DIR, запакуем директорию tmp в архив tmp.zip."]},{"cell_type":"code","metadata":{"id":"CRwgPtv-6nMP"},"source":["'''arr1 = np.random.rand(100, 100, 3) * 255\r\n","arr2 = np.random.rand(100, 100, 3) * 255\r\n","\r\n","img1 = Image.fromarray(arr1.astype('uint8'))\r\n","img2 = Image.fromarray(arr2.astype('uint8'))\r\n","\r\n","p = \"/content/drive/MyDrive/\" + PROJECT_DIR\r\n","\r\n","if not (Path(p) / 'tmp').exists():\r\n","    (Path(p) / 'tmp').mkdir()\r\n","\r\n","img1.save(str(Path(p) / 'tmp' / 'img1.png'))\r\n","img2.save(str(Path(p) / 'tmp' / 'img2.png'))\r\n","\r\n","%cd $p\r\n","!zip -r \"tmp.zip\" \"tmp\"'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MykrBSWNQQlq"},"source":["Распакуем архив tmp.zip в директорию tmp2 в PROJECT_DIR. Теперь внутри директории tmp2 содержится директория tmp, внутри которой находятся 2 изображения."]},{"cell_type":"code","metadata":{"id":"CwSWrYIWMAus"},"source":["'''p = \"/content/drive/MyDrive/\" + PROJECT_DIR\r\n","%cd $p\r\n","!unzip -uq \"tmp.zip\" -d \"tmp2\"'''"],"execution_count":null,"outputs":[]}]}