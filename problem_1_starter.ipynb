{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atqZGIIyNSBb"
   },
   "source": [
    "#**Практическое задание №1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ga5g3lUhNNBy"
   },
   "source": [
    "Установка необходимых пакетов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5640,
     "status": "ok",
     "timestamp": 1612632716924,
     "user": {
      "displayName": "Live It",
      "photoUrl": "",
      "userId": "01143106410518725341"
     },
     "user_tz": -180
    },
    "id": "TGBk36LpukIu"
   },
   "outputs": [],
   "source": [
    "!pip install -q libtiff\n",
    "!pip install -q tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vQDLyHEO1Ux"
   },
   "source": [
    "Монтирование Вашего Google Drive к текущему окружению:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6624,
     "status": "ok",
     "timestamp": 1612632717925,
     "user": {
      "displayName": "Live It",
      "photoUrl": "",
      "userId": "01143106410518725341"
     },
     "user_tz": -180
    },
    "id": "5G5KkA1Nu5M9",
    "outputId": "371984fe-c23a-4557-8294-295710c32312"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive', force_remount=True)\n",
    "base_dir='./'\n",
    "#base_dir ='/drive/MyDrive/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6-mtI6W1y1b"
   },
   "source": [
    "В переменную PROJECT_DIR необходимо прописать путь к директории на Google Drive, в которую Вы загрузили zip архивы с предоставленными наборами данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 6615,
     "status": "ok",
     "timestamp": 1612632717926,
     "user": {
      "displayName": "Live It",
      "photoUrl": "",
      "userId": "01143106410518725341"
     },
     "user_tz": -180
    },
    "id": "IdvM-BUTvfSV"
   },
   "outputs": [],
   "source": [
    "# todo\n",
    "PROJECT_DIR='./'\n",
    "#PROJECT_DIR = 'Хвостиков_задание/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Num5lHV6912"
   },
   "source": [
    "Константы, которые пригодятся в коде далее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6609,
     "status": "ok",
     "timestamp": 1612632717927,
     "user": {
      "displayName": "Live It",
      "photoUrl": "",
      "userId": "01143106410518725341"
     },
     "user_tz": -180
    },
    "id": "ab2yCwDm7Fqb"
   },
   "outputs": [],
   "source": [
    "EVALUATE_ONLY = False\n",
    "TEST_ON_LARGE_DATASET = False\n",
    "TISSUE_CLASSES = ('ADI', 'BACK', 'DEB', 'LYM', 'MUC', 'MUS', 'NORM', 'STR', 'TUM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgY-ux5qOI0k"
   },
   "source": [
    "Импорт необходимых зависимостей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7544,
     "status": "ok",
     "timestamp": 1612632718870,
     "user": {
      "displayName": "Live It",
      "photoUrl": "",
      "userId": "01143106410518725341"
     },
     "user_tz": -180
    },
    "id": "kLHQhqiSIyvK"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from libtiff import TIFF\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from tqdm.notebook import tqdm\n",
    "from time import sleep\n",
    "from PIL import Image\n",
    "import IPython.display\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKLI3lUyMYO9"
   },
   "source": [
    "---\n",
    "### Класс Dataset\n",
    "\n",
    "Предназначен для работы с наборами данных, хранящихся на Google Drive, обеспечивает чтение изображений и соответствующих меток, а также формирование пакетов (батчей)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 7538,
     "status": "ok",
     "timestamp": 1612632718874,
     "user": {
      "displayName": "Live It",
      "photoUrl": "",
      "userId": "01143106410518725341"
     },
     "user_tz": -180
    },
    "id": "8N169efsw1ej"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "\n",
    "    def __init__(self, name, gdrive_dir):\n",
    "        self.name = name\n",
    "        self.is_loaded = False\n",
    "        p = Path(base_dir + gdrive_dir + name + '.npz') \n",
    "        if p.exists():\n",
    "            print(f'Loading dataset {self.name} from npz.')\n",
    "            np_obj = np.load(str(p))\n",
    "            self.images = np_obj['data']\n",
    "            self.labels = np_obj['labels']\n",
    "            self.n_files = self.images.shape[0]\n",
    "            self.is_loaded = True\n",
    "            print(f'Done. Dataset {name} consists of {self.n_files} images.')\n",
    "        else:\n",
    "          print(f'WARNING: path {str(p)} doesn\\'t exist')\n",
    "\n",
    "    def image(self, i):\n",
    "        # read i-th image in dataset and return it as numpy array\n",
    "        if self.is_loaded:\n",
    "            return self.images[i, :, :, :]\n",
    "\n",
    "    def images_seq(self, start=0, n=None):\n",
    "        # sequential access to images inside dataset (is needed for testing)\n",
    "        for i in range(start, start + self.n_files if not n else start + n):\n",
    "            yield self.image(i)\n",
    "\n",
    "    def random_image_with_label(self):\n",
    "        # get random image with label from dataset\n",
    "        i = np.random.randint(self.n_files)\n",
    "        return self.image(i), self.labels[i]\n",
    "  \n",
    "    def random_batch_with_labels(self, n):\n",
    "        # create random batch of images with labels (is needed for training)\n",
    "        indices = np.random.choice(self.n_files, n)\n",
    "        imgs = []\n",
    "        for i in indices:\n",
    "            img = self.image(i)\n",
    "            imgs.append(self.image(i))\n",
    "        logits = np.array([self.labels[i] for i in indices])\n",
    "        return np.stack(imgs), logits\n",
    "\n",
    "    def image_with_label(self, i: int):\n",
    "        # return i-th image with label from dataset\n",
    "        return self.image(i), self.labels[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-LvGqeHYgus"
   },
   "source": [
    "### Пример использвания класса Dataset\n",
    "Загрузим обучающий набор данных, получим произвольное изображение с меткой. После чего визуализируем изображение, выведем метку. В будущем, этот кусок кода можно закомментировать или убрать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 7524,
     "status": "ok",
     "timestamp": 1612632718875,
     "user": {
      "displayName": "Live It",
      "photoUrl": "",
      "userId": "01143106410518725341"
     },
     "user_tz": -180
    },
    "id": "HhObWEjGJ1um",
    "outputId": "339f2eac-58ab-4b6b-e033-3c944f1339f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"d_train_tiny = Dataset('train_tiny', PROJECT_DIR)\\n\\nimg, lbl = d_train_tiny.random_image_with_label()\\nprint()\\nprint(f'Got numpy array of shape {img.shape}, and label with code {lbl}.')\\nprint(f'Label code corresponds to {TISSUE_CLASSES[lbl]} class.')\\n\\npil_img = Image.fromarray(img)\\nIPython.display.display(pil_img)\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''d_train_tiny = Dataset('train_tiny', PROJECT_DIR)\n",
    "\n",
    "img, lbl = d_train_tiny.random_image_with_label()\n",
    "print()\n",
    "print(f'Got numpy array of shape {img.shape}, and label with code {lbl}.')\n",
    "print(f'Label code corresponds to {TISSUE_CLASSES[lbl]} class.')\n",
    "\n",
    "pil_img = Image.fromarray(img)\n",
    "IPython.display.display(pil_img)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaBXXCWeVLYb"
   },
   "source": [
    "---\n",
    "### Класс Metrics\n",
    "\n",
    "Реализует метрики точности, используемые для оценивания модели:\n",
    "1. точность,\n",
    "2. сбалансированную точность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 7512,
     "status": "ok",
     "timestamp": 1612632718876,
     "user": {
      "displayName": "Live It",
      "photoUrl": "",
      "userId": "01143106410518725341"
     },
     "user_tz": -180
    },
    "id": "5unQ7azTinCZ"
   },
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(gt: List[int], pred: List[int]):\n",
    "        assert len(gt) == len(pred), 'gt and prediction should be of equal length'\n",
    "        print([f'{i[0]} : {i[1]}' for i in list(zip(gt, pred))[:5]])\n",
    "        return sum(int(i[0] == i[1]) for i in zip(gt, pred)) / len(gt)\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy_balanced(gt: List[int], pred: List[int]):\n",
    "        return balanced_accuracy_score(gt, pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def print_all(gt: List[int], pred: List[int], info: str):\n",
    "        print(f'metrics for {info}:')\n",
    "        print('\\t accuracy {:.4f}:'.format(Metrics.accuracy(gt, pred)))\n",
    "        print('\\t balanced accuracy {:.4f}:'.format(Metrics.accuracy_balanced(gt, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1AHzTJVGU5k"
   },
   "source": [
    "---\n",
    "### Класс Model\n",
    "\n",
    "Класс, хранящий в себе всю информацию о модели.\n",
    "\n",
    "Вам необходимо реализовать методы save, load для сохранения и заргрузки модели. Особенно актуально это будет во время тестирования на дополнительных наборах данных.\n",
    "\n",
    "> *Пожалуйста, убедитесь, что сохранение и загрузка модели работает корректно. Для этого обучите модель, протестируйте, сохраните ее в файл, перезапустите среду выполнения, загрузите обученную модель из файла, вновь протестируйте ее на тестовой выборке и убедитесь в том, что получаемые метрики совпадают с полученными для тестовой выбрки ранее.*\n",
    "\n",
    "\n",
    "Также, Вы можете реализовать дополнительные функции, такие как:\n",
    "1. валидацию модели на части обучающей выборки;\n",
    "2. использование кроссвалидации;\n",
    "3. автоматическое сохранение модели при обучении;\n",
    "4. загрузку модели с какой-то конкретной итерации обучения (если используется итеративное обучение);\n",
    "5. вывод различных показателей в процессе обучения (например, значение функции потерь на каждой эпохе);\n",
    "6. построение графиков, визуализирующих процесс обучения (например, график зависимости функции потерь от номера эпохи обучения);\n",
    "7. автоматическое тестирование на тестовом наборе/наборах данных после каждой эпохи обучения (при использовании итеративного обучения);\n",
    "8. автоматический выбор гиперпараметров модели во время обучения;\n",
    "9. сохранение и визуализацию результатов тестирования;\n",
    "10. Использование аугментации и других способов синтетического расширения набора данных (дополнительным плюсом будет обоснование необходимости и обоснование выбора конкретных типов аугментации)\n",
    "11. и т.д.\n",
    "\n",
    "Полный список опций и дополнений приведен в презентации с описанием задания.\n",
    "\n",
    "При реализации дополнительных функций допускается добавление параметров в существующие методы и добавление новых методов в класс модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 13859,
     "status": "ok",
     "timestamp": 1612632725232,
     "user": {
      "displayName": "Live It",
      "photoUrl": "",
      "userId": "01143106410518725341"
     },
     "user_tz": -180
    },
    "id": "0pkMiB6mJ7JQ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import cv2\n",
    "import os\n",
    "from random import random, randint\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Using architecture from https://openaccess.thecvf.com/content_cvpr_2016/papers/Hou_Patch-Based_Convolutional_Neural_CVPR_2016_paper.pdf\n",
    "        \n",
    "        self.input_shape=(224, 224, 3)\n",
    "        self.model = tf.keras.Sequential([\n",
    "          tf.keras.layers.Conv2D(filters=80, kernel_size=5, strides=1, input_shape=self.input_shape, kernel_regularizer=l2(.0005)),\n",
    "          tf.keras.layers.BatchNormalization(),\n",
    "          tf.keras.layers.ReLU(),\n",
    "          tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=3),\n",
    "\n",
    "          tf.keras.layers.Conv2D(filters=120, kernel_size=5, kernel_regularizer=l2(.0005)),\n",
    "          tf.keras.layers.BatchNormalization(),\n",
    "          tf.keras.layers.ReLU(),\n",
    "          tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
    "\n",
    "          tf.keras.layers.Conv2D(filters=160, kernel_size=3, kernel_regularizer=l2(.0005)),\n",
    "          tf.keras.layers.ReLU(),\n",
    "\n",
    "          tf.keras.layers.Conv2D(filters=200, kernel_size=3, kernel_regularizer=l2(.0005)),\n",
    "          tf.keras.layers.ReLU(),\n",
    "          tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
    "\n",
    "          tf.keras.layers.Flatten(),\n",
    "\n",
    "          tf.keras.layers.Dense(units=256, activation='relu'),\n",
    "          tf.keras.layers.Dropout(rate=0.5),\n",
    "\n",
    "          tf.keras.layers.Dense(units=256, activation='relu'),\n",
    "          tf.keras.layers.Dropout(rate=0.5),\n",
    "\n",
    "          tf.keras.layers.Dense(units=9, activation='softmax')\n",
    "        ])\n",
    "        #self.model = tf.keras.applications.InceptionV3(classes=9, weights=None)\n",
    "\n",
    "        self.save_path=base_dir + PROJECT_DIR + 'best.h5'\n",
    "        self.checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=self.save_path,\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        self.model.compile(optimizer='adam',\n",
    "                           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                           metrics=['accuracy'])\n",
    "        \n",
    "        self.clf = SGDClassifier()\n",
    "        #self.clf = make_pipeline(StandardScaler(), SGDClassifier())\n",
    "\n",
    "    def summary(self):\n",
    "      print(self.model.summary())\n",
    "\n",
    "    def save(self, name: str):\n",
    "        # save model to PROJECT_DIR folder on gdrive with name 'name'\n",
    "        # todo\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def load(name: str):\n",
    "        # load model with name 'name' from PROJECT_DIR folder on gdrive\n",
    "        # todo\n",
    "        model = Model()\n",
    "        if os.path.exists(base_dir + PROJECT_DIR + f'{name}.h5'):\n",
    "          model.model = tf.keras.models.load_model(base_dir + PROJECT_DIR + f'{name}.h5')\n",
    "        return model\n",
    "\n",
    "    def train(self, dataset: Dataset):\n",
    "        # you can add some plots for better visualization,\n",
    "        # you can add model autosaving during training,\n",
    "        # etc.\n",
    "        print(f'training started')\n",
    "        \n",
    "        for i in range(70):\n",
    "\n",
    "          n = 500\n",
    "          x_train, y_train = dataset.random_batch_with_labels(n)\n",
    "          x_train = self.preproc_batch(x_train)\n",
    "\n",
    "          x_val, y_val = dataset.random_batch_with_labels(150)\n",
    "          x_val = self.preproc_batch(x_val, 0)\n",
    "\n",
    "          print(x_train.shape, y_train.shape)\n",
    "          self.model.fit(x_train, y_train, epochs=30, callbacks=[self.checkpoint_callback], validation_data=(x_val, y_val))\n",
    "\n",
    "        print(f'training done')\n",
    "        pass\n",
    "\n",
    "    def train_clf(self, dataset: Dataset):\n",
    "      print('training_clf started')\n",
    "\n",
    "      intermediate_layer = 'dropout_1'\n",
    "      self.feature_extractor = tf.keras.models.Model(inputs=self.model.input, outputs=self.model.get_layer(intermediate_layer).output)\n",
    "\n",
    "      for i in tqdm(range(70)):\n",
    "\n",
    "        n = 250\n",
    "        x_train, y_train = dataset.random_batch_with_labels(n)\n",
    "        x_train = self.preproc_batch(x_train)\n",
    "        print(x_train.shape)\n",
    "        x_train = self.feature_extractor.predict(x_train)\n",
    "\n",
    "        print(x_train.shape)\n",
    "\n",
    "        self.clf.partial_fit(x_train, y_train, classes=np.arange(9))\n",
    "\n",
    "      print('training_clf done')\n",
    "\n",
    "    def preproc_batch(self, batch, thr=.5):\n",
    "      imgs = []\n",
    "      for image in batch:\n",
    "        if random() < thr:\n",
    "          border = randint(10, 40)\n",
    "          image = image[border:-border, border:-border]\n",
    "        img = cv2.resize(image/255., self.input_shape[:2], interpolation=cv2.INTER_CUBIC)\n",
    "        if random() < thr:\n",
    "          img = img[:, ::-1]\n",
    "        if random() < thr:\n",
    "          img = img[::-1]\n",
    "        '''cv2.imshow('image', img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()'''\n",
    "        imgs.append(img)\n",
    "\n",
    "      return np.stack(imgs)\n",
    "\n",
    "    def test_on_dataset(self, dataset: Dataset, limit=None, use_clf=False):\n",
    "        # you can upgrade this code if you want to speed up testing using batches\n",
    "        predictions = []\n",
    "        n = dataset.n_files if not limit else int(dataset.n_files * limit)\n",
    "        batch_size = 100\n",
    "        current = 0\n",
    "        for current in tqdm(range(0, n, min(batch_size, n - current))):\n",
    "          for img in self.preproc_batch(dataset.images_seq(current, batch_size)):\n",
    "            predictions.append(self.test_on_image(img[np.newaxis, :], use_clf))\n",
    "        return predictions\n",
    "\n",
    "    def test_on_image(self, img: np.ndarray, use_clf=False):\n",
    "      if use_clf:\n",
    "        prediction = self.clf.predict(self.feature_extractor.predict(img))\n",
    "        return prediction\n",
    "      else:\n",
    "        prediction = self.model.predict(img)\n",
    "        return np.argmax(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMpTB6lMr00A"
   },
   "source": [
    "---\n",
    "### Классификация изображений\n",
    "\n",
    "Используя введенные выше классы можем перейти уже непосредственно к обучению модели классификации изображений. Пример общего пайплайна решения задачи приведен ниже. Вы можете его расширять и улучшать. В данном примере используются наборы данных 'train_small' и 'test_small'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86321,
     "status": "ok",
     "timestamp": 1612632797705,
     "user": {
      "displayName": "Live It",
      "photoUrl": "",
      "userId": "01143106410518725341"
     },
     "user_tz": -180
    },
    "id": "5cTOuZD01Up6",
    "outputId": "e0d6d981-a020-41ad-f1c9-6b894e82dafa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset train from npz.\n",
      "Done. Dataset train consists of 18000 images.\n",
      "Loading dataset test from npz.\n",
      "Done. Dataset test consists of 4500 images.\n"
     ]
    }
   ],
   "source": [
    "d_train = Dataset('train', PROJECT_DIR)\n",
    "d_test = Dataset('test', PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wBi0XpXg8_wq",
    "outputId": "3c27f7a0-a2d2-49a7-c431-79c26db066ed",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 220, 220, 80)      6080      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 220, 220, 80)      320       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 220, 220, 80)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 73, 73, 80)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 69, 69, 120)       240120    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 69, 69, 120)       480       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 69, 69, 120)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 34, 34, 120)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 160)       172960    \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 32, 32, 160)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 30, 30, 200)       288200    \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 30, 30, 200)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 200)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 39200)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               10035456  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 2313      \n",
      "=================================================================\n",
      "Total params: 10,811,721\n",
      "Trainable params: 10,811,321\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n",
      "None\n",
      "training started\n",
      "(500, 224, 224, 3) (500,)\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.4463 - accuracy: 0.1200\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.12000, saving model to ././best.h5\n",
      "16/16 [==============================] - 9s 535ms/step - loss: 2.4463 - accuracy: 0.1200 - val_loss: 2.4428 - val_accuracy: 0.1200\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.4112 - accuracy: 0.1400\n",
      "Epoch 00002: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.4112 - accuracy: 0.1400 - val_loss: 2.4175 - val_accuracy: 0.1200\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.3928 - accuracy: 0.1340\n",
      "Epoch 00003: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.3928 - accuracy: 0.1340 - val_loss: 2.3947 - val_accuracy: 0.1200\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.3693 - accuracy: 0.1360\n",
      "Epoch 00004: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 2.3693 - accuracy: 0.1360 - val_loss: 2.3746 - val_accuracy: 0.1200\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.3505 - accuracy: 0.1360\n",
      "Epoch 00005: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.3505 - accuracy: 0.1360 - val_loss: 2.3573 - val_accuracy: 0.1200\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.3343 - accuracy: 0.1360\n",
      "Epoch 00006: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.3343 - accuracy: 0.1360 - val_loss: 2.3425 - val_accuracy: 0.1200\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.3185 - accuracy: 0.1380\n",
      "Epoch 00007: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 2.3185 - accuracy: 0.1380 - val_loss: 2.3299 - val_accuracy: 0.1200\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.3148 - accuracy: 0.1300\n",
      "Epoch 00008: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 2.3148 - accuracy: 0.1300 - val_loss: 2.3192 - val_accuracy: 0.1200\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.3068 - accuracy: 0.1280\n",
      "Epoch 00009: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.3068 - accuracy: 0.1280 - val_loss: 2.3100 - val_accuracy: 0.1200\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2922 - accuracy: 0.1340\n",
      "Epoch 00010: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.2922 - accuracy: 0.1340 - val_loss: 2.3020 - val_accuracy: 0.1200\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2828 - accuracy: 0.1360\n",
      "Epoch 00011: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 2.2828 - accuracy: 0.1360 - val_loss: 2.2952 - val_accuracy: 0.1200\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2725 - accuracy: 0.1400\n",
      "Epoch 00012: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.2725 - accuracy: 0.1400 - val_loss: 2.2894 - val_accuracy: 0.1200\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2730 - accuracy: 0.1340\n",
      "Epoch 00013: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 2.2730 - accuracy: 0.1340 - val_loss: 2.2844 - val_accuracy: 0.1200\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2663 - accuracy: 0.1360\n",
      "Epoch 00014: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.2663 - accuracy: 0.1360 - val_loss: 2.2801 - val_accuracy: 0.1200\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2643 - accuracy: 0.1340\n",
      "Epoch 00015: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.2643 - accuracy: 0.1340 - val_loss: 2.2764 - val_accuracy: 0.1200\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2689 - accuracy: 0.1260\n",
      "Epoch 00016: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.2689 - accuracy: 0.1260 - val_loss: 2.2732 - val_accuracy: 0.1200\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2582 - accuracy: 0.1340\n",
      "Epoch 00017: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.2582 - accuracy: 0.1340 - val_loss: 2.2726 - val_accuracy: 0.1200\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2562 - accuracy: 0.1360\n",
      "Epoch 00018: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 2.2562 - accuracy: 0.1360 - val_loss: 2.2713 - val_accuracy: 0.1200\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2542 - accuracy: 0.1360\n",
      "Epoch 00019: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 2.2542 - accuracy: 0.1360 - val_loss: 2.2690 - val_accuracy: 0.1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2520 - accuracy: 0.1360\n",
      "Epoch 00020: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.2520 - accuracy: 0.1360 - val_loss: 2.2668 - val_accuracy: 0.1200\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2499 - accuracy: 0.1360\n",
      "Epoch 00021: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 2.2499 - accuracy: 0.1360 - val_loss: 2.2649 - val_accuracy: 0.1200\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2502 - accuracy: 0.1340\n",
      "Epoch 00022: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.2502 - accuracy: 0.1340 - val_loss: 2.2633 - val_accuracy: 0.1200\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2467 - accuracy: 0.1360\n",
      "Epoch 00023: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.2467 - accuracy: 0.1360 - val_loss: 2.2620 - val_accuracy: 0.1200\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2434 - accuracy: 0.1380\n",
      "Epoch 00024: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 2.2434 - accuracy: 0.1380 - val_loss: 2.2608 - val_accuracy: 0.1200\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2423 - accuracy: 0.1380\n",
      "Epoch 00025: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.2423 - accuracy: 0.1380 - val_loss: 2.2598 - val_accuracy: 0.1200\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2434 - accuracy: 0.1360\n",
      "Epoch 00026: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.2434 - accuracy: 0.1360 - val_loss: 2.2589 - val_accuracy: 0.1200\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2425 - accuracy: 0.1360\n",
      "Epoch 00027: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.2425 - accuracy: 0.1360 - val_loss: 2.2581 - val_accuracy: 0.1200\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2418 - accuracy: 0.1360\n",
      "Epoch 00028: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.2418 - accuracy: 0.1360 - val_loss: 2.2574 - val_accuracy: 0.1200\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2411 - accuracy: 0.1360\n",
      "Epoch 00029: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.2411 - accuracy: 0.1360 - val_loss: 2.2568 - val_accuracy: 0.1200\n",
      "Epoch 30/30\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 2.2390 - accuracy: 0.1375\n",
      "Epoch 00030: val_accuracy did not improve from 0.12000\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 2.2405 - accuracy: 0.1360 - val_loss: 2.2563 - val_accuracy: 0.1200\n",
      "(500, 224, 224, 3) (500,)\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2384 - accuracy: 0.1380\n",
      "Epoch 00001: val_accuracy improved from 0.12000 to 0.16667, saving model to ././best.h5\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 2.2384 - accuracy: 0.1380 - val_loss: 2.2118 - val_accuracy: 0.1667\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2529 - accuracy: 0.1280\n",
      "Epoch 00002: val_accuracy did not improve from 0.16667\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 2.2529 - accuracy: 0.1280 - val_loss: 2.2157 - val_accuracy: 0.1667\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2539 - accuracy: 0.1280\n",
      "Epoch 00003: val_accuracy did not improve from 0.16667\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.2539 - accuracy: 0.1280 - val_loss: 2.2144 - val_accuracy: 0.1667\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2483 - accuracy: 0.1320\n",
      "Epoch 00004: val_accuracy did not improve from 0.16667\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.2483 - accuracy: 0.1320 - val_loss: 2.2127 - val_accuracy: 0.1667\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.2507 - accuracy: 0.1280\n",
      "Epoch 00005: val_accuracy did not improve from 0.16667\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 2.2507 - accuracy: 0.1280 - val_loss: 2.2113 - val_accuracy: 0.1667\n",
      "Epoch 6/30\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 2.2448 - accuracy: 0.1328"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-be7b27bfd440>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mEVALUATE_ONLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m#model.train_clf(d_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-14459c79d3f9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'training done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/niki2/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/niki2/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/niki2/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \"\"\"\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/niki2/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_process_logs\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;34m\"\"\"Turns tensors into numpy arrays or Python scalars.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/niki2/lib/python3.8/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/niki2/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/niki2/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/niki2/lib/python3.8/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    517\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/niki2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \"\"\"\n\u001b[1;32m    960\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/niki2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Model.load('best')\n",
    "model.summary()\n",
    "if not EVALUATE_ONLY:\n",
    "  #model.train_clf(d_train)\n",
    "  model.train(d_train)\n",
    "\n",
    "  model.load('best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcM2EiRMVP93"
   },
   "source": [
    "Пример тестирования модели на части набора данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0AqmeLEKqrs"
   },
   "outputs": [],
   "source": [
    "# evaluating model on x% of test dataset\n",
    "limit=1.\n",
    "pred_1 = model.test_on_dataset(d_test, limit=limit)\n",
    "Metrics.print_all(d_test.labels[:len(pred_1)], pred_1, f'{limit*100}% of test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSwvHVVzVWZ5"
   },
   "source": [
    "Пример тестирования модели на полном наборе данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjI_sbMi3TMY"
   },
   "outputs": [],
   "source": [
    "# evaluating model on full test dataset (may take time)\n",
    "if TEST_ON_LARGE_DATASET:\n",
    "    pred_2 = model.test_on_dataset(d_test)\n",
    "    Metrics.print_all(d_test.labels, pred_2, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvyEHdxEB18o"
   },
   "source": [
    "Результат работы пайплайна обучения и тестирования выше тоже будет оцениваться. Поэтому не забудьте присылать на проверку ноутбук с выполнеными ячейками кода с демонстрациями метрик обучения, графиками и т.п. В этом пайплайне Вам необходимо продемонстрировать работу всех реализованных дополнений, улучшений и т.п.\n",
    "\n",
    "<font color=\"red\">\n",
    "Настоятельно рекомендуется после получения пайплайна с полными результатами обучения экспортировать ноутбук в pdf (файл -> печать) и прислать этот pdf вместе с самим ноутбуком.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzSKAvVI6uCW"
   },
   "source": [
    "### Тестирование модели на других наборах данных\n",
    "\n",
    "Ваша модель должна поддерживать тестирование на других наборах данных. Для удобства, Вам предоставляется набор данных test_tiny, который представляет собой малую часть (2% изображений) набора test. Ниже приведен фрагмент кода, который будет осуществлять тестирование для оценивания Вашей модели на дополнительных тестовых наборах данных.\n",
    "\n",
    "<font color=\"red\">\n",
    "Прежде чем отсылать задание на проверку, убедитесь в работоспособности фрагмента кода ниже.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdY3uTt87tqv"
   },
   "outputs": [],
   "source": [
    "final_model = Model()\n",
    "final_model.load('best')\n",
    "d_test_tiny = Dataset('test_tiny', PROJECT_DIR)\n",
    "pred = model.test_on_dataset(d_test_tiny)\n",
    "Metrics.print_all(d_test_tiny.labels, pred, 'test-tiny')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPvyj4gscU10"
   },
   "source": [
    "Отмонтировать Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfX35zNSvFWn"
   },
   "outputs": [],
   "source": [
    "#drive.flush_and_unmount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMyDxCDCspcI"
   },
   "source": [
    "---\n",
    "# Дополнительные \"полезности\"\n",
    "\n",
    "Ниже приведены примеры использования различных функций и библиотек, которые могут быть полезны при выполнении данного практического задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvLwSttCs1rB"
   },
   "source": [
    "### Измерение времени работы кода\n",
    "\n",
    "Измерять время работы какой-либо функции можно легко и непринужденно при помощи функции timeit из соответствующего модуля:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HnLVhwE9C9S"
   },
   "outputs": [],
   "source": [
    "'''import timeit\n",
    "\n",
    "def factorial(n):\n",
    "    res = 1\n",
    "    for i in range(1, n + 1):\n",
    "        res *= i\n",
    "    return res\n",
    "\n",
    "\n",
    "def f():\n",
    "    return factorial(n=1000)\n",
    "\n",
    "n_runs = 128\n",
    "print(f'Function f is caluclated {n_runs} times in {timeit.timeit(f, number=n_runs)}s.')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fibGVEdguOOi"
   },
   "source": [
    "### Scikit-learn\n",
    "\n",
    "Для использования \"классических\" алгоритмов машинного обучения рекомендуется использовать библиотеку scikit-learn (https://scikit-learn.org/stable/). Пример классификации изображений цифр из набора данных MNIST при помощи классификатора SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXHnBzEfunAO"
   },
   "outputs": [],
   "source": [
    "'''# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# The data that we are interested in is made of 8x8 images of digits, let's\n",
    "# have a look at the first 4 images, stored in the `images` attribute of the\n",
    "# dataset.  If we were working from image files, we could load them using\n",
    "# matplotlib.pyplot.imread.  Note that each image must have the same size. For these\n",
    "# images, we know which digit they represent: it is given in the 'target' of\n",
    "# the dataset.\n",
    "_, axes = plt.subplots(2, 4)\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for ax, (image, label) in zip(axes[0, :], images_and_labels[:4]):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Training: %i' % label)\n",
    "\n",
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Create a classifier: a support vector classifier\n",
    "classifier = svm.SVC(gamma=0.001)\n",
    "\n",
    "# Split data into train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, digits.target, test_size=0.5, shuffle=False)\n",
    "\n",
    "# We learn the digits on the first half of the digits\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Now predict the value of the digit on the second half:\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\n",
    "for ax, (image, prediction) in zip(axes[1, :], images_and_predictions[:4]):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Prediction: %i' % prediction)\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(y_test, predicted)))\n",
    "disp = metrics.plot_confusion_matrix(classifier, X_test, y_test)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "print(\"Confusion matrix:\\n%s\" % disp.confusion_matrix)\n",
    "\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uu3Dny5zxcVy"
   },
   "source": [
    "### Scikit-image\n",
    "\n",
    "Реализовывать различные операции для работы с изображениями можно как самостоятельно, работая с массивами numpy, так и используя специализированные библиотеки, например, scikit-image (https://scikit-image.org/). Ниже приведен пример использования Canny edge detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TZvy_d7xc0B"
   },
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from skimage import feature\n",
    "\n",
    "\n",
    "# Generate noisy image of a square\n",
    "im = np.zeros((128, 128))\n",
    "im[32:-32, 32:-32] = 1\n",
    "\n",
    "im = ndi.rotate(im, 15, mode='constant')\n",
    "im = ndi.gaussian_filter(im, 4)\n",
    "im += 0.2 * np.random.random(im.shape)\n",
    "\n",
    "# Compute the Canny filter for two values of sigma\n",
    "edges1 = feature.canny(im)\n",
    "edges2 = feature.canny(im, sigma=3)\n",
    "\n",
    "# display results\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(8, 3),\n",
    "                                    sharex=True, sharey=True)\n",
    "\n",
    "ax1.imshow(im, cmap=plt.cm.gray)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('noisy image', fontsize=20)\n",
    "\n",
    "ax2.imshow(edges1, cmap=plt.cm.gray)\n",
    "ax2.axis('off')\n",
    "ax2.set_title(r'Canny filter, $\\sigma=1$', fontsize=20)\n",
    "\n",
    "ax3.imshow(edges2, cmap=plt.cm.gray)\n",
    "ax3.axis('off')\n",
    "ax3.set_title(r'Canny filter, $\\sigma=3$', fontsize=20)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiEWhGUQRGoH"
   },
   "source": [
    "### Tensorflow 2\n",
    "\n",
    "Для создания и обучения нейросетевых моделей можно использовать фреймворк глубокого обучения Tensorflow 2. Ниже приведен пример простейшей нейроной сети, использующейся для классификации изображений из набора данных MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDwLG7A1ReNy"
   },
   "outputs": [],
   "source": [
    "'''# Install TensorFlow\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbvktmLwRu8g"
   },
   "source": [
    "<font color=\"red\">\n",
    "Для эффективной работы с моделями глубокого обучения убедитесь в том, что в текущей среде Google Colab используется аппаратный ускоритель GPU или TPU. Для смены среды выберите \"среда выполнения\" -> \"сменить среду выполнения\".\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJVNOOU9Sjyf"
   },
   "source": [
    "Большое количество туториалов и примеров с кодом на Tensorflow 2 можно найти на официальном сайте https://www.tensorflow.org/tutorials?hl=ru. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVPs3pYpS0U1"
   },
   "source": [
    "Также, Вам может понадобиться написать собственный генератор данных для Tensorflow 2. Скорее всего он будет достаточно простым, и его легко можно будет реализовать, используя официальную документацию TensorFlow 2. Но, на всякий случай (если не удлось сразу разобраться или хочется вникнуть в тему более глубоко), можете посмотреть следующий отличный туториал: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwI-T0IXyN84"
   },
   "source": [
    "### Numba\n",
    "\n",
    "В некоторых ситуациях, при ручных реализациях графовых алгоритмов, выполнение многократных вложенных циклов for в python можно существенно ускорить, используя JIT-компилятор Numba (https://numba.pydata.org/).\n",
    "Примеры использования Numba в Google Colab можно найти тут:\n",
    "1. https://colab.research.google.com/github/cbernet/maldives/blob/master/numba/numba_cuda.ipynb\n",
    "2. https://colab.research.google.com/github/evaneschneider/parallel-programming/blob/master/COMPASS_gpu_intro.ipynb \n",
    "\n",
    "> Пожалуйста, если Вы решили использовать Numba для решения этого практического задания, еще раз подумайте, нужно ли это Вам, и есть ли возможность реализовать требуемую функциональность иным способом. Используйте Numba только при реальной необходимости.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxAJ00A76LcF"
   },
   "source": [
    "### Работа с zip архивами в Google Drive\n",
    "\n",
    "Запаковка и распаковка zip архивов может пригодиться при сохранении и загрузки Вашей модели. Ниже приведен фрагмент кода, иллюстрирующий помещение нескольких файлов в zip архив с последующим чтением файлов из него. Все действия с директориями, файлами и архивами должны осущетвляться с примонтированным Google Drive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJiKndOpPu_e"
   },
   "source": [
    "Создадим 2 изображения, поместим их в директорию tmp внутри PROJECT_DIR, запакуем директорию tmp в архив tmp.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRwgPtv-6nMP"
   },
   "outputs": [],
   "source": [
    "'''arr1 = np.random.rand(100, 100, 3) * 255\n",
    "arr2 = np.random.rand(100, 100, 3) * 255\n",
    "\n",
    "img1 = Image.fromarray(arr1.astype('uint8'))\n",
    "img2 = Image.fromarray(arr2.astype('uint8'))\n",
    "\n",
    "p = \"/content/drive/MyDrive/\" + PROJECT_DIR\n",
    "\n",
    "if not (Path(p) / 'tmp').exists():\n",
    "    (Path(p) / 'tmp').mkdir()\n",
    "\n",
    "img1.save(str(Path(p) / 'tmp' / 'img1.png'))\n",
    "img2.save(str(Path(p) / 'tmp' / 'img2.png'))\n",
    "\n",
    "%cd $p\n",
    "!zip -r \"tmp.zip\" \"tmp\"'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MykrBSWNQQlq"
   },
   "source": [
    "Распакуем архив tmp.zip в директорию tmp2 в PROJECT_DIR. Теперь внутри директории tmp2 содержится директория tmp, внутри которой находятся 2 изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwSWrYIWMAus"
   },
   "outputs": [],
   "source": [
    "'''p = \"/content/drive/MyDrive/\" + PROJECT_DIR\n",
    "%cd $p\n",
    "!unzip -uq \"tmp.zip\" -d \"tmp2\"'''"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "problem_1_starter.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
