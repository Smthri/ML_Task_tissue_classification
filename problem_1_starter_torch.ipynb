{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atqZGIIyNSBb"
   },
   "source": [
    "#**Практическое задание №1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ga5g3lUhNNBy"
   },
   "source": [
    "Установка необходимых пакетов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5640,
     "status": "ok",
     "timestamp": 1612632716924,
     "user": {
      "displayName": "Live It",
      "photoUrl": "",
      "userId": "01143106410518725341"
     },
     "user_tz": -180
    },
    "id": "TGBk36LpukIu"
   },
   "outputs": [],
   "source": [
    "!pip install -q libtiff\n",
    "!pip install -q tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vQDLyHEO1Ux"
   },
   "source": [
    "Монтирование Вашего Google Drive к текущему окружению:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6624,
     "status": "ok",
     "timestamp": 1612632717925,
     "user": {
      "displayName": "Live It",
      "photoUrl": "",
      "userId": "01143106410518725341"
     },
     "user_tz": -180
    },
    "id": "5G5KkA1Nu5M9",
    "outputId": "371984fe-c23a-4557-8294-295710c32312"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive', force_remount=True)\n",
    "base_dir='./'\n",
    "#base_dir ='/drive/MyDrive/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6-mtI6W1y1b"
   },
   "source": [
    "В переменную PROJECT_DIR необходимо прописать путь к директории на Google Drive, в которую Вы загрузили zip архивы с предоставленными наборами данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6615,
     "status": "ok",
     "timestamp": 1612632717926,
     "user": {
      "displayName": "Live It",
      "photoUrl": "",
      "userId": "01143106410518725341"
     },
     "user_tz": -180
    },
    "id": "IdvM-BUTvfSV"
   },
   "outputs": [],
   "source": [
    "# todo\n",
    "PROJECT_DIR='./'\n",
    "#PROJECT_DIR = 'Хвостиков_задание/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Num5lHV6912"
   },
   "source": [
    "Константы, которые пригодятся в коде далее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 6609,
     "status": "ok",
     "timestamp": 1612632717927,
     "user": {
      "displayName": "Live It",
      "photoUrl": "",
      "userId": "01143106410518725341"
     },
     "user_tz": -180
    },
    "id": "ab2yCwDm7Fqb"
   },
   "outputs": [],
   "source": [
    "EVALUATE_ONLY = False\n",
    "TEST_ON_LARGE_DATASET = False\n",
    "TISSUE_CLASSES = ('ADI', 'BACK', 'DEB', 'LYM', 'MUC', 'MUS', 'NORM', 'STR', 'TUM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgY-ux5qOI0k"
   },
   "source": [
    "Импорт необходимых зависимостей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 7544,
     "status": "ok",
     "timestamp": 1612632718870,
     "user": {
      "displayName": "Live It",
      "photoUrl": "",
      "userId": "01143106410518725341"
     },
     "user_tz": -180
    },
    "id": "kLHQhqiSIyvK"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from libtiff import TIFF\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from tqdm.notebook import tqdm\n",
    "from time import sleep\n",
    "from PIL import Image\n",
    "import IPython.display\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKLI3lUyMYO9"
   },
   "source": [
    "---\n",
    "### Класс Dataset\n",
    "\n",
    "Предназначен для работы с наборами данных, хранящихся на Google Drive, обеспечивает чтение изображений и соответствующих меток, а также формирование пакетов (батчей)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7538,
     "status": "ok",
     "timestamp": 1612632718874,
     "user": {
      "displayName": "Live It",
      "photoUrl": "",
      "userId": "01143106410518725341"
     },
     "user_tz": -180
    },
    "id": "8N169efsw1ej"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, name, gdrive_dir, transforms=None):\n",
    "        self.name = name\n",
    "        self.is_loaded = False\n",
    "        p = Path(base_dir + gdrive_dir + name + '.npz') \n",
    "        if p.exists():\n",
    "            print(f'Loading dataset {self.name} from npz.')\n",
    "            np_obj = np.load(str(p))\n",
    "            self.images = np_obj['data']\n",
    "            self.labels = np_obj['labels']\n",
    "            self.n_files = self.images.shape[0]\n",
    "            self.is_loaded = True\n",
    "            print(f'Done. Dataset {name} consists of {self.n_files} images.')\n",
    "        else:\n",
    "          print(f'WARNING: path {str(p)} doesn\\'t exist')\n",
    "        \n",
    "        self.transforms = transforms\n",
    "\n",
    "    def image(self, i):\n",
    "        # read i-th image in dataset and return it as numpy array\n",
    "        if self.is_loaded:\n",
    "            return self.images[i, :, :, :]\n",
    "\n",
    "    def images_seq(self, start=0, n=None):\n",
    "        # sequential access to images inside dataset (is needed for testing)\n",
    "        for i in range(start, start + self.n_files if not n else start + n):\n",
    "            yield self.image(i)\n",
    "\n",
    "    def random_image_with_label(self):\n",
    "        # get random image with label from dataset\n",
    "        i = np.random.randint(self.n_files)\n",
    "        return self.image(i), self.labels[i]\n",
    "  \n",
    "    def random_batch_with_labels(self, n):\n",
    "        # create random batch of images with labels (is needed for training)\n",
    "        indices = np.random.choice(self.n_files, n)\n",
    "        imgs = []\n",
    "        for i in indices:\n",
    "            img = self.image(i)\n",
    "            imgs.append(self.image(i))\n",
    "        logits = np.array([self.labels[i] for i in indices])\n",
    "        return np.stack(imgs), logits\n",
    "\n",
    "    def image_with_label(self, i: int):\n",
    "        # return i-th image with label from dataset\n",
    "        return self.image(i), self.labels[i]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # return i-th image with label from dataset\n",
    "        sample = {'image': self.image(i), 'label': self.labels[i]}\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_files\n",
    "    \n",
    "# Transformations\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image/255.).float(),\n",
    "                'label': int(label)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaBXXCWeVLYb"
   },
   "source": [
    "---\n",
    "### Класс Metrics\n",
    "\n",
    "Реализует метрики точности, используемые для оценивания модели:\n",
    "1. точность,\n",
    "2. сбалансированную точность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 7512,
     "status": "ok",
     "timestamp": 1612632718876,
     "user": {
      "displayName": "Live It",
      "photoUrl": "",
      "userId": "01143106410518725341"
     },
     "user_tz": -180
    },
    "id": "5unQ7azTinCZ"
   },
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(gt: List[int], pred: List[int]):\n",
    "        assert len(gt) == len(pred), 'gt and prediction should be of equal length'\n",
    "        print([f'{i[0]} : {i[1]}' for i in list(zip(gt, pred))[:5]])\n",
    "        return sum(int(i[0] == i[1]) for i in zip(gt, pred)) / len(gt)\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy_balanced(gt: List[int], pred: List[int]):\n",
    "        return balanced_accuracy_score(gt, pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def print_all(gt: List[int], pred: List[int], info: str):\n",
    "        print(f'metrics for {info}:')\n",
    "        print('\\t accuracy {:.4f}:'.format(Metrics.accuracy(gt, pred)))\n",
    "        print('\\t balanced accuracy {:.4f}:'.format(Metrics.accuracy_balanced(gt, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1AHzTJVGU5k"
   },
   "source": [
    "---\n",
    "### Класс Model\n",
    "\n",
    "Класс, хранящий в себе всю информацию о модели.\n",
    "\n",
    "Вам необходимо реализовать методы save, load для сохранения и заргрузки модели. Особенно актуально это будет во время тестирования на дополнительных наборах данных.\n",
    "\n",
    "> *Пожалуйста, убедитесь, что сохранение и загрузка модели работает корректно. Для этого обучите модель, протестируйте, сохраните ее в файл, перезапустите среду выполнения, загрузите обученную модель из файла, вновь протестируйте ее на тестовой выборке и убедитесь в том, что получаемые метрики совпадают с полученными для тестовой выбрки ранее.*\n",
    "\n",
    "\n",
    "Также, Вы можете реализовать дополнительные функции, такие как:\n",
    "1. валидацию модели на части обучающей выборки;\n",
    "2. использование кроссвалидации;\n",
    "3. автоматическое сохранение модели при обучении;\n",
    "4. загрузку модели с какой-то конкретной итерации обучения (если используется итеративное обучение);\n",
    "5. вывод различных показателей в процессе обучения (например, значение функции потерь на каждой эпохе);\n",
    "6. построение графиков, визуализирующих процесс обучения (например, график зависимости функции потерь от номера эпохи обучения);\n",
    "7. автоматическое тестирование на тестовом наборе/наборах данных после каждой эпохи обучения (при использовании итеративного обучения);\n",
    "8. автоматический выбор гиперпараметров модели во время обучения;\n",
    "9. сохранение и визуализацию результатов тестирования;\n",
    "10. Использование аугментации и других способов синтетического расширения набора данных (дополнительным плюсом будет обоснование необходимости и обоснование выбора конкретных типов аугментации)\n",
    "11. и т.д.\n",
    "\n",
    "Полный список опций и дополнений приведен в презентации с описанием задания.\n",
    "\n",
    "При реализации дополнительных функций допускается добавление параметров в существующие методы и добавление новых методов в класс модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 13859,
     "status": "ok",
     "timestamp": 1612632725232,
     "user": {
      "displayName": "Live It",
      "photoUrl": "",
      "userId": "01143106410518725341"
     },
     "user_tz": -180
    },
    "id": "0pkMiB6mJ7JQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from random import random, randint\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.input_shape=(224, 224, 3)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=80, kernel_size=5),\n",
    "            nn.BatchNorm2d(num_features=80),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=5, stride=1),\n",
    "            \n",
    "            nn.Conv2d(80, 120, 5),\n",
    "            nn.BatchNorm2d(120),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2),\n",
    "            \n",
    "            nn.Conv2d(120, 160, 3),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(160, 200, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(in_features=500000, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=.5),\n",
    "            \n",
    "            nn.Linear(in_features=256, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.5),\n",
    "            \n",
    "            nn.Linear(in_features=256, out_features=9),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(torch.Tensor(x))\n",
    "    \n",
    "    def preproc_batch(self, batch, thr=.5):\n",
    "      imgs = []\n",
    "      for image in batch:\n",
    "        if random() < thr:\n",
    "          border = randint(10, 40)\n",
    "          image = image[border:-border, border:-border]\n",
    "        img = cv2.resize(image/255., self.input_shape[:2], interpolation=cv2.INTER_CUBIC)\n",
    "        if random() < thr:\n",
    "          img = img[:, ::-1]\n",
    "        if random() < thr:\n",
    "          img = img[::-1]\n",
    "        '''cv2.imshow('image', img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()'''\n",
    "        imgs.append(img.transpose(2, 0, 1))\n",
    "\n",
    "      return np.stack(imgs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(name: str):\n",
    "        # load model with name 'name' from PROJECT_DIR folder on gdrive\n",
    "        # todo\n",
    "        model = Model()\n",
    "        if os.path.exists(base_dir + PROJECT_DIR + f'{name}.pth'):\n",
    "            model.model = torch.load(base_dir + PROJECT_DIR + f'{name}.pth')\n",
    "        return model\n",
    "    \n",
    "    def train(self, trainloader):\n",
    "        print(f'training started')\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters())\n",
    "            \n",
    "        epochs = 1\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            current_loss = 0.\n",
    "            print(f'Epoch {epoch}/{epochs}')\n",
    "            \n",
    "            pbar = tqdm(enumerate(trainloader, 0), total=len(trainloader))\n",
    "            for i, data in pbar:\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = self.model(data['image'].to(self.device))\n",
    "                loss = criterion(outputs, data['label'].to(self.device))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                current_loss = loss.item()\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                pbar.set_description(f'loss: {current_loss}')\n",
    "                \n",
    "            print(f'loss: {running_loss/i}')\n",
    "\n",
    "        print(f'training done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMpTB6lMr00A"
   },
   "source": [
    "---\n",
    "### Классификация изображений\n",
    "\n",
    "Используя введенные выше классы можем перейти уже непосредственно к обучению модели классификации изображений. Пример общего пайплайна решения задачи приведен ниже. Вы можете его расширять и улучшать. В данном примере используются наборы данных 'train_small' и 'test_small'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86321,
     "status": "ok",
     "timestamp": 1612632797705,
     "user": {
      "displayName": "Live It",
      "photoUrl": "",
      "userId": "01143106410518725341"
     },
     "user_tz": -180
    },
    "id": "5cTOuZD01Up6",
    "outputId": "e0d6d981-a020-41ad-f1c9-6b894e82dafa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset train_small from npz.\n",
      "Done. Dataset train_small consists of 7200 images.\n",
      "Loading dataset test_small from npz.\n",
      "Done. Dataset test_small consists of 1800 images.\n"
     ]
    }
   ],
   "source": [
    "d_train = Dataset('train_small', PROJECT_DIR, ToTensor())\n",
    "d_test = Dataset('test_small', PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(d_train, batch_size=500, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wBi0XpXg8_wq",
    "outputId": "3c27f7a0-a2d2-49a7-c431-79c26db066ed",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 80, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=5, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(80, 120, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(120, 160, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(160, 200, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (13): Flatten(start_dim=1, end_dim=-1)\n",
      "    (14): Linear(in_features=500000, out_features=256, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Dropout(p=0.5, inplace=False)\n",
      "    (17): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (18): ReLU()\n",
      "    (19): Dropout(p=0.5, inplace=False)\n",
      "    (20): Linear(in_features=256, out_features=9, bias=True)\n",
      "    (21): Softmax(dim=None)\n",
      "  )\n",
      ")\n",
      "training started\n",
      "Epoch 0/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356314f789d24c358910251b0f68098e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 7.21 GiB (GPU 0; 10.92 GiB total capacity; 7.97 GiB already allocated; 1.97 GiB free; 7.99 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-66c1091f3532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mEVALUATE_ONLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m#model.train_clf(d_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b4a564996bf4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainloader)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/niki2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/niki2/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/niki2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/niki2/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \"\"\"\n\u001b[0;32m--> 131\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/niki2/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2054\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2056\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2057\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 7.21 GiB (GPU 0; 10.92 GiB total capacity; 7.97 GiB already allocated; 1.97 GiB free; 7.99 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "model = Model.load('best')\n",
    "print(str(model))\n",
    "if not EVALUATE_ONLY:\n",
    "  #model.train_clf(d_train)\n",
    "  model.train(trainloader)\n",
    "\n",
    "  model.load('best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcM2EiRMVP93"
   },
   "source": [
    "Пример тестирования модели на части набора данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0AqmeLEKqrs"
   },
   "outputs": [],
   "source": [
    "# evaluating model on x% of test dataset\n",
    "limit=1.\n",
    "pred_1 = model.test_on_dataset(d_test, limit=limit)\n",
    "Metrics.print_all(d_test.labels[:len(pred_1)], pred_1, f'{limit*100}% of test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSwvHVVzVWZ5"
   },
   "source": [
    "Пример тестирования модели на полном наборе данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjI_sbMi3TMY"
   },
   "outputs": [],
   "source": [
    "# evaluating model on full test dataset (may take time)\n",
    "if TEST_ON_LARGE_DATASET:\n",
    "    pred_2 = model.test_on_dataset(d_test)\n",
    "    Metrics.print_all(d_test.labels, pred_2, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvyEHdxEB18o"
   },
   "source": [
    "Результат работы пайплайна обучения и тестирования выше тоже будет оцениваться. Поэтому не забудьте присылать на проверку ноутбук с выполнеными ячейками кода с демонстрациями метрик обучения, графиками и т.п. В этом пайплайне Вам необходимо продемонстрировать работу всех реализованных дополнений, улучшений и т.п.\n",
    "\n",
    "<font color=\"red\">\n",
    "Настоятельно рекомендуется после получения пайплайна с полными результатами обучения экспортировать ноутбук в pdf (файл -> печать) и прислать этот pdf вместе с самим ноутбуком.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzSKAvVI6uCW"
   },
   "source": [
    "### Тестирование модели на других наборах данных\n",
    "\n",
    "Ваша модель должна поддерживать тестирование на других наборах данных. Для удобства, Вам предоставляется набор данных test_tiny, который представляет собой малую часть (2% изображений) набора test. Ниже приведен фрагмент кода, который будет осуществлять тестирование для оценивания Вашей модели на дополнительных тестовых наборах данных.\n",
    "\n",
    "<font color=\"red\">\n",
    "Прежде чем отсылать задание на проверку, убедитесь в работоспособности фрагмента кода ниже.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdY3uTt87tqv"
   },
   "outputs": [],
   "source": [
    "final_model = Model()\n",
    "final_model.load('best')\n",
    "d_test_tiny = Dataset('test_tiny', PROJECT_DIR)\n",
    "pred = model.test_on_dataset(d_test_tiny)\n",
    "Metrics.print_all(d_test_tiny.labels, pred, 'test-tiny')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPvyj4gscU10"
   },
   "source": [
    "Отмонтировать Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfX35zNSvFWn"
   },
   "outputs": [],
   "source": [
    "#drive.flush_and_unmount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMyDxCDCspcI"
   },
   "source": [
    "---\n",
    "# Дополнительные \"полезности\"\n",
    "\n",
    "Ниже приведены примеры использования различных функций и библиотек, которые могут быть полезны при выполнении данного практического задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvLwSttCs1rB"
   },
   "source": [
    "### Измерение времени работы кода\n",
    "\n",
    "Измерять время работы какой-либо функции можно легко и непринужденно при помощи функции timeit из соответствующего модуля:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HnLVhwE9C9S"
   },
   "outputs": [],
   "source": [
    "'''import timeit\n",
    "\n",
    "def factorial(n):\n",
    "    res = 1\n",
    "    for i in range(1, n + 1):\n",
    "        res *= i\n",
    "    return res\n",
    "\n",
    "\n",
    "def f():\n",
    "    return factorial(n=1000)\n",
    "\n",
    "n_runs = 128\n",
    "print(f'Function f is caluclated {n_runs} times in {timeit.timeit(f, number=n_runs)}s.')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fibGVEdguOOi"
   },
   "source": [
    "### Scikit-learn\n",
    "\n",
    "Для использования \"классических\" алгоритмов машинного обучения рекомендуется использовать библиотеку scikit-learn (https://scikit-learn.org/stable/). Пример классификации изображений цифр из набора данных MNIST при помощи классификатора SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXHnBzEfunAO"
   },
   "outputs": [],
   "source": [
    "'''# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# The data that we are interested in is made of 8x8 images of digits, let's\n",
    "# have a look at the first 4 images, stored in the `images` attribute of the\n",
    "# dataset.  If we were working from image files, we could load them using\n",
    "# matplotlib.pyplot.imread.  Note that each image must have the same size. For these\n",
    "# images, we know which digit they represent: it is given in the 'target' of\n",
    "# the dataset.\n",
    "_, axes = plt.subplots(2, 4)\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for ax, (image, label) in zip(axes[0, :], images_and_labels[:4]):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Training: %i' % label)\n",
    "\n",
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Create a classifier: a support vector classifier\n",
    "classifier = svm.SVC(gamma=0.001)\n",
    "\n",
    "# Split data into train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, digits.target, test_size=0.5, shuffle=False)\n",
    "\n",
    "# We learn the digits on the first half of the digits\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Now predict the value of the digit on the second half:\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\n",
    "for ax, (image, prediction) in zip(axes[1, :], images_and_predictions[:4]):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Prediction: %i' % prediction)\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(y_test, predicted)))\n",
    "disp = metrics.plot_confusion_matrix(classifier, X_test, y_test)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "print(\"Confusion matrix:\\n%s\" % disp.confusion_matrix)\n",
    "\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uu3Dny5zxcVy"
   },
   "source": [
    "### Scikit-image\n",
    "\n",
    "Реализовывать различные операции для работы с изображениями можно как самостоятельно, работая с массивами numpy, так и используя специализированные библиотеки, например, scikit-image (https://scikit-image.org/). Ниже приведен пример использования Canny edge detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TZvy_d7xc0B"
   },
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from skimage import feature\n",
    "\n",
    "\n",
    "# Generate noisy image of a square\n",
    "im = np.zeros((128, 128))\n",
    "im[32:-32, 32:-32] = 1\n",
    "\n",
    "im = ndi.rotate(im, 15, mode='constant')\n",
    "im = ndi.gaussian_filter(im, 4)\n",
    "im += 0.2 * np.random.random(im.shape)\n",
    "\n",
    "# Compute the Canny filter for two values of sigma\n",
    "edges1 = feature.canny(im)\n",
    "edges2 = feature.canny(im, sigma=3)\n",
    "\n",
    "# display results\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(8, 3),\n",
    "                                    sharex=True, sharey=True)\n",
    "\n",
    "ax1.imshow(im, cmap=plt.cm.gray)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('noisy image', fontsize=20)\n",
    "\n",
    "ax2.imshow(edges1, cmap=plt.cm.gray)\n",
    "ax2.axis('off')\n",
    "ax2.set_title(r'Canny filter, $\\sigma=1$', fontsize=20)\n",
    "\n",
    "ax3.imshow(edges2, cmap=plt.cm.gray)\n",
    "ax3.axis('off')\n",
    "ax3.set_title(r'Canny filter, $\\sigma=3$', fontsize=20)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiEWhGUQRGoH"
   },
   "source": [
    "### Tensorflow 2\n",
    "\n",
    "Для создания и обучения нейросетевых моделей можно использовать фреймворк глубокого обучения Tensorflow 2. Ниже приведен пример простейшей нейроной сети, использующейся для классификации изображений из набора данных MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDwLG7A1ReNy"
   },
   "outputs": [],
   "source": [
    "'''# Install TensorFlow\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbvktmLwRu8g"
   },
   "source": [
    "<font color=\"red\">\n",
    "Для эффективной работы с моделями глубокого обучения убедитесь в том, что в текущей среде Google Colab используется аппаратный ускоритель GPU или TPU. Для смены среды выберите \"среда выполнения\" -> \"сменить среду выполнения\".\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJVNOOU9Sjyf"
   },
   "source": [
    "Большое количество туториалов и примеров с кодом на Tensorflow 2 можно найти на официальном сайте https://www.tensorflow.org/tutorials?hl=ru. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVPs3pYpS0U1"
   },
   "source": [
    "Также, Вам может понадобиться написать собственный генератор данных для Tensorflow 2. Скорее всего он будет достаточно простым, и его легко можно будет реализовать, используя официальную документацию TensorFlow 2. Но, на всякий случай (если не удлось сразу разобраться или хочется вникнуть в тему более глубоко), можете посмотреть следующий отличный туториал: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwI-T0IXyN84"
   },
   "source": [
    "### Numba\n",
    "\n",
    "В некоторых ситуациях, при ручных реализациях графовых алгоритмов, выполнение многократных вложенных циклов for в python можно существенно ускорить, используя JIT-компилятор Numba (https://numba.pydata.org/).\n",
    "Примеры использования Numba в Google Colab можно найти тут:\n",
    "1. https://colab.research.google.com/github/cbernet/maldives/blob/master/numba/numba_cuda.ipynb\n",
    "2. https://colab.research.google.com/github/evaneschneider/parallel-programming/blob/master/COMPASS_gpu_intro.ipynb \n",
    "\n",
    "> Пожалуйста, если Вы решили использовать Numba для решения этого практического задания, еще раз подумайте, нужно ли это Вам, и есть ли возможность реализовать требуемую функциональность иным способом. Используйте Numba только при реальной необходимости.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxAJ00A76LcF"
   },
   "source": [
    "### Работа с zip архивами в Google Drive\n",
    "\n",
    "Запаковка и распаковка zip архивов может пригодиться при сохранении и загрузки Вашей модели. Ниже приведен фрагмент кода, иллюстрирующий помещение нескольких файлов в zip архив с последующим чтением файлов из него. Все действия с директориями, файлами и архивами должны осущетвляться с примонтированным Google Drive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJiKndOpPu_e"
   },
   "source": [
    "Создадим 2 изображения, поместим их в директорию tmp внутри PROJECT_DIR, запакуем директорию tmp в архив tmp.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRwgPtv-6nMP"
   },
   "outputs": [],
   "source": [
    "'''arr1 = np.random.rand(100, 100, 3) * 255\n",
    "arr2 = np.random.rand(100, 100, 3) * 255\n",
    "\n",
    "img1 = Image.fromarray(arr1.astype('uint8'))\n",
    "img2 = Image.fromarray(arr2.astype('uint8'))\n",
    "\n",
    "p = \"/content/drive/MyDrive/\" + PROJECT_DIR\n",
    "\n",
    "if not (Path(p) / 'tmp').exists():\n",
    "    (Path(p) / 'tmp').mkdir()\n",
    "\n",
    "img1.save(str(Path(p) / 'tmp' / 'img1.png'))\n",
    "img2.save(str(Path(p) / 'tmp' / 'img2.png'))\n",
    "\n",
    "%cd $p\n",
    "!zip -r \"tmp.zip\" \"tmp\"'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MykrBSWNQQlq"
   },
   "source": [
    "Распакуем архив tmp.zip в директорию tmp2 в PROJECT_DIR. Теперь внутри директории tmp2 содержится директория tmp, внутри которой находятся 2 изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwSWrYIWMAus"
   },
   "outputs": [],
   "source": [
    "'''p = \"/content/drive/MyDrive/\" + PROJECT_DIR\n",
    "%cd $p\n",
    "!unzip -uq \"tmp.zip\" -d \"tmp2\"'''"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "problem_1_starter.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
